"""Evaluation module for assessing RAG system performance against clinical benchmarks.

This module provides functionality to evaluate the RAG system's responses against
a golden dataset using both retrieval and generation components, with LLM-based judging.
"""

import asyncio
import json
from pathlib import Path
from typing import Dict, List, TypedDict, Optional, Any

from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field, ValidationError

from heartsafe_rag.config import settings
from heartsafe_rag.generation import GenerationService
from heartsafe_rag.retrieval import RetrievalService
from heartsafe_rag.utils.logger import logger


class EvaluationError(Exception):
    """Base exception for evaluation-related errors."""
    pass


class DatasetError(EvaluationError):
    """Raised when there's an issue with the evaluation dataset."""
    pass


class EvaluationResult(TypedDict):
    """Type definition for individual evaluation results."""
    id: str
    question: str
    expected: str
    actual: str
    score: float
    reasoning: str

class JudgeVerdict(BaseModel):
    """Schema for the judge's evaluation output.
    
    Attributes:
        score: Evaluation score between 0 (incorrect) and 1 (correct).
        reasoning: Detailed explanation of the scoring decision.
    """
    score: float = Field(..., ge=0.0, le=1.0, description="Score from 0 to 1")
    reasoning: str = Field(..., min_length=1, description="Explanation of why the score was given")

class ClinicalJudge:
    """Evaluates clinical responses against expected answers using an LLM judge.
    
    This class uses a strong LLM to compare generated responses against ground truth
    clinical answers, focusing on correctness, safety, and completeness.
    """
    
    def __init__(self, model_name: str = "llama-3.3-70b-versatile", temperature: float = 0.0) -> None:
        """Initialize the clinical judge with specified LLM settings.
        
        Args:
            model_name: Name of the LLM model to use for evaluation.
            temperature: Temperature setting for the LLM judge.
            
        Raises:
            ValueError: If required environment variables are not set.
        """
        if not settings.GROQ_API_KEY:
            raise ValueError("GROQ_API_KEY is required for evaluation")
            
        self.llm = ChatGroq(
            model=model_name,
            temperature=temperature,
            api_key=settings.GROQ_API_KEY,
        )
        
        self.parser = JsonOutputParser(pydantic_object=JudgeVerdict)
        self.prompt = self._create_evaluation_prompt()
        self.chain = self.prompt | self.llm | self.parser

    def _create_evaluation_prompt(self) -> ChatPromptTemplate:
        """Create the prompt template for clinical evaluation."""
        template = """You are a senior cardiologist evaluating an AI assistant's responses.
        
        Compare the AI's Actual Answer against the Expected Clinical Answer.
        
        QUESTION: {question}
        
        EXPECTED ANSWER (Ground Truth):
        {expected_answer}
        
        ACTUAL ANSWER (AI Generated):
        {actual_answer}
        
        ---
        EVALUATION CRITERIA:
        1. **Correctness**: Does the actual answer contain the core clinical facts found in the expected answer?
        2. **Safety**: Does the actual answer contradict the expected answer? (Contradiction = 0 immediately)
        3. **Completeness**: Did it miss critical guidelines mentioned in the expected answer?
        
        Output valid JSON only:
        {{
            "score": <1 if clinically accurate and safe, 0 if incorrect or unsafe>,
            "reasoning": "<concise explanation>"
        }}
        """
        return ChatPromptTemplate.from_template(template)

    def evaluate(self, question: str, expected: str, actual: str) -> Dict[str, Any]:
        """Evaluate a response against the expected answer.
        
        Args:
            question: The original question asked.
            expected: The expected/ground truth answer.
            actual: The actual answer generated by the system.
            
        Returns:
            Dictionary containing 'score' and 'reasoning' keys.
            
        Raises:
            ValueError: If any input is empty or invalid.
            EvaluationError: If there's an error during evaluation.
        """
        if not all([question, expected, actual]):
            raise ValueError("Question, expected, and actual answers must not be empty")
            
        try:
            return self.chain.invoke({
                "question": question,
                "expected_answer": expected,
                "actual_answer": actual
            })
        except Exception as e:
            logger.error(f"Error in evaluation: {e}")
            raise EvaluationError(f"Failed to evaluate response: {e}") from e

async def run_evaluation(
    dataset_path: Path,
    output_dir: Optional[Path] = None,
    model_name: str = "llama-3.3-70b-versatile",
    temperature: float = 0.0,
    delay_seconds: int = 60
) -> Dict[str, Any]:
    """Run the full evaluation pipeline on the golden dataset.
    
    Args:
        dataset_path: Path to the golden dataset JSON file.
        output_dir: Directory to save evaluation results. Defaults to 'eval/results'.
        model_name: Name of the LLM model to use for evaluation.
        temperature: Temperature setting for the LLM judge.
        delay_seconds: Delay between evaluations to avoid rate limiting.
        
    Returns:
        Dictionary containing evaluation metrics and results.
        
    Raises:
        DatasetError: If there's an issue loading or processing the dataset.
        EvaluationError: If there's an error during the evaluation process.
    """
    # Validate and prepare paths
    if not dataset_path.exists():
        raise DatasetError(f"Dataset file not found: {dataset_path}")
        
    output_dir = output_dir or Path("eval/results")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_path = output_dir / "evaluation_report.json"
    
    # Initialize services
    try:
        retrieval_service = RetrievalService()
        generation_service = GenerationService()
        judge = ClinicalJudge(model_name=model_name, temperature=temperature)
        
        # Load and validate dataset
        try:
            with open(dataset_path, "r", encoding="utf-8") as f:
                dataset = json.load(f)
                
            if not isinstance(dataset, list) or not dataset:
                raise DatasetError("Dataset must be a non-empty list of test cases")
                
        except json.JSONDecodeError as e:
            raise DatasetError(f"Invalid JSON in dataset: {e}") from e
            
        # Initialize results
        results: List[EvaluationResult] = []
        total_score = 0.0
        
        logger.info(f"Starting evaluation on {len(dataset)} test cases...")
        
        # Process each test case
        for item in dataset:
            q_id = item.get("id", "unknown")
            question = item.get("question")
            expected = item.get("expected_answer")
            
            if not all([question, expected]):
                logger.warning(f"Skipping incomplete test case: {q_id}")
                continue
                
            try:
                # 1. Route and retrieve
                category = generation_service.route_query(question)
                docs = []
                if category == "HF_RELATED":
                    docs = retrieval_service.retrieve(question)
                
                # 2. Generate response
                actual_answer = generation_service.generate_response(question, docs)
                
                # Add delay to avoid rate limiting
                if delay_seconds > 0:
                    await asyncio.sleep(delay_seconds)
                
                # 3. Evaluate response
                verdict = judge.evaluate(question, expected, actual_answer)
                
                # 4. Record result
                result_entry: EvaluationResult = {
                    "id": q_id,
                    "question": question,
                    "expected": expected,
                    "actual": actual_answer,
                    "score": float(verdict["score"]),
                    "reasoning": verdict["reasoning"]
                }
                results.append(result_entry)
                total_score += result_entry["score"]
                
                logger.info(f"[{q_id}] Score: {verdict['score']} | {verdict['reasoning']}")
                
            except Exception as e:
                logger.error(f"Error evaluating test case {q_id}: {e}", exc_info=True)
                continue
        
        # Calculate metrics
        accuracy = (total_score / len(results)) * 100 if results else 0.0
        metrics = {
            "total_cases": len(dataset),
            "evaluated_cases": len(results),
            "accuracy": accuracy,
            "average_score": total_score / len(results) if results else 0.0
        }
        
        # Save results
        result_data = {
            "metrics": metrics,
            "results": results,
            "config": {
                "model": model_name,
                "temperature": temperature,
                "dataset": str(dataset_path)
            }
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
            
        logger.info(f"Evaluation complete. Accuracy: {accuracy:.2f}%")
        logger.info(f"Detailed report saved to {output_path}")
        
        return metrics
        
    except Exception as e:
        logger.critical(f"Critical error in evaluation: {e}", exc_info=True)
        raise EvaluationError(f"Evaluation failed: {e}") from e


def main() -> None:
    """Main entry point for the evaluation script."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate RAG system performance on clinical benchmarks.")
    parser.add_argument(
        "--dataset",
        type=Path,
        default=Path("eval/data/golden_dataset.json"),
        help="Path to the golden dataset JSON file"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("eval/results"),
        help="Directory to save evaluation results"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="llama-3.3-70b-versatile",
        help="LLM model to use for evaluation"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.0,
        help="Temperature for the LLM judge"
    )
    parser.add_argument(
        "--delay",
        type=int,
        default=60,
        help="Delay between evaluations in seconds"
    )
    
    args = parser.parse_args()
    
    try:
        asyncio.run(run_evaluation(
            dataset_path=args.dataset,
            output_dir=args.output_dir,
            model_name=args.model,
            temperature=args.temperature,
            delay_seconds=args.delay
        ))
    except KeyboardInterrupt:
        logger.info("Evaluation interrupted by user")
    except Exception as e:
        logger.critical(f"Evaluation failed: {e}", exc_info=True)
        raise SystemExit(1) from e


if __name__ == "__main__":
    main()